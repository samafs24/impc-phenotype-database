---
title: "DCDM Project Log"
author: "DCDM Group 3"
editor: visual
format: html
---

# TRE Egress

## **Data Security Awareness Training**

1.  Account created through the [NHS e-Learning for Healthcare portal](https://portal.e-lfh.org.uk/Component/Details/544034)
2.  Course *Data Security Awareness - Level 1* :
    -   Completed the training and passed the examination

## **CREATE TRE Egress**

1.  Logged on to the Virtual Machine
    -   This [link](https://portal.er.kcl.ac.uk/vdiresource/281) was used to access a virtual machine (specific machine: er-tre-352-vm07) through the KCL e-research portal er_tre_msc_dcdm Virtual Desktops
    -   Logged into Virtual Machine with my KCL login
2.  From the Desktop, clicked on the "Data_Egress_Request‚Äù icon, which lead to the KCL TRE Data Egress Portal
3.  Clicked on the "Egress request tab", which led to a form to fill out with details about the data requested
    -   Selected "Group3.zip" within the file picker, requesting our specific dataset
    -   Answered questions about the classification, point of origin, end-point destination, purpose for egress, and type of data, specific to our dataset
4.  Submitted the Egress Request
5.  The egress was approved, and the data was made available to us as a zip file (Group3.zip) on the CREATE HPC in the /scratch_tmp/grp/msc_appbio/7BBG1003/CW folder

## Moving the Data

Though we had access to the zip file of our data, we had no permissions on it so were unable to move or copy it to our own data. This required me to download the file to my own system using scp, then re-upload the file to a our group project folder using scp. I checked the file sizes before and after using the `ls -l` command, to ensure no data was corrupted in the process.

##### `ls -l` does not detect corrupted files; it lists file metadata like permissions, size, and modification date but does not verify file integrity. Use checksums like `md5sum` or `sha256sum` instead to verify file integrity. {style="color: red"}

```{bash}
# downloading the data locally from the HPC
# scp -i ~/.ssh/msckey  k24064085@hpc.create.kcl.ac.uk:/scratch_tmp/grp/msc_appbio/7BBG1003/CW/Group3.zip ~/Desktop/applied_bioinformatics/DCDM/group_project/raw_data/Group3

# uploading the data into our DCDM_group3 directory on the HPC
# scp -i ~/.ssh/msckey  ~/Desktop/applied_bioinformatics/DCDM/group_project/raw_data/Group3.zip k24064085@hpc.create.kcl.ac.uk:/sratch_tmp/grp/msc_appbio/DCDM_group3
```

# Data Cleaning

```{r}
# Clear workspace
rm(list = ls())

# Install packages if needed
# install.packages(c("tidyverse", "janitor", "fs"))

# Load packages
library(tidyverse)  # Data manipulation
library(janitor)    # Data cleaning
library(fs)         # File system operations
```

```{r}
#| warning: false

# Set working directory (uncomment for other users as needed)
rootDir <- "/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/" # Allie
# rootDir <- "/Users/sanjanasrinivasan/Desktop/DCDM_IMPC_Project/" # Sanj
# rootDir <- "/Users/ellajadealex/Desktop/applied_bioinformatics/DCDM/group_project/DCDM_IMPC_Project/" # Ella
# rootDir <- "/Users/sama/Desktop/DCDM_Proj2/" # Sama
# rootDir <- "~/Desktop/working_directory/DCDM_project" # Esta

# Create directories
dir.create(file.path(rootDir, "data/metadata"), recursive = TRUE)      # Directory for procedure, parameter, disease files, query genes, and SOP documents
dir.create(file.path(rootDir, "data/raw_data"), recursive = TRUE)      # Directory for storing raw analysis data files
dir.create(file.path(rootDir, "data/updated_data"), recursive = TRUE)  # Directory for storing raw data files with corrected field names
dir.create(file.path(rootDir, "data/cleaned"), recursive = TRUE)       # Directory for cleaned metadata tables and a single combined table of all analyses
dir.create(file.path(rootDir, "data/collated"), recursive = TRUE)      # Directory for tables ready to be uploaded into the database
dir.create(file.path(rootDir, "data/database"), recursive = TRUE)      # Directory for the database dump file
```

## Cleaning the Raw Data Files

The experiment data includes an SOP that defines constraints for each field, such as data type, minimum and maximum values for numerical fields, string lengths, and specific allowable values. Data quality is assessed by comparing the raw data against these constraints.

In this step, we have cleaned all CSV files by modifying the row names to match those specified in the SOP.

We begin by defining the directory paths and the log file path.

```{r}
# Define directory paths
raw_data_dir <- paste0(rootDir, "data/raw_data/")          # Directory for input raw data files
updated_data_dir <- paste0(rootDir, "data/updated_data/")  # Directory for updated raw data files with corrected field names

# Define file paths
log_file <- paste0(rootDir, "scripts/analysis_cleaning_overall.log")  # Log file for recording validation and modifications
SOP_path <- paste0(rootDir, "data/metadata/IMPC_SOP.csv")             # Path to the SOP CSV file

# Create the log file or clear its contents if it already exists
file.create(log_file)
```

Then we load the SOP file.

```{r}
# Load the SOP file and extract the expected row names, trimming whitespace
expected_row_names <- trimws(read_csv(SOP_path, show_col_types = FALSE)$dataField)

# Display the normalized row names
expected_row_names
```

### Validating the Number of Columns

To quickly identify any any delimitation errors, we can validate the number of columns per CSV file (each file should have 2 columns).

```{r}
# Function to validate the column count in a file
validate_columns <- function(file_path) {
  # Attempt to read the file, returning NULL if an error occurs
  analysis <- tryCatch(
    read_csv(file_path, col_names = FALSE, show_col_types = FALSE),
    error = function(e) return(NULL)  # Handle file reading errors gracefully
  )
  
  # Return FALSE if the file could not be read
  if (is.null(analysis)) return(FALSE)
  
  # Check if the file has exactly 2 columns; return TRUE if it does, otherwise FALSE
  return(ncol(analysis) == 2)
}
```

### Check and Correct the Field Names

With the correct files loaded and directories specified, we define a function to perform the following tasks:

1.  Identify and correct spelling mistakes in the row names. In this case, only uppercase-to-lowercase transformations are needed;

2.  Add any missing rows, assigning `NA` to the second column for those rows. This step is performed after correcting any misspelled row names;

3.  Reorder the rows to align with the SOP layout.

Here, I have also included the column count validation function defined above. This means the results will all be presented in the same file.

```{r}
# Function to validate, update, and reorder row names
check_files <- function(file_path) {
  # Validate column count
  if(!validate_columns(file_path)){
    message(paste("File", basename(file_path), "does not have exactly 2 columns. Skipping."))
    return(FALSE)
  }
  # Read the CSV file as headerless
  analysis <- tryCatch(
    read_csv(file_path, col_names = FALSE, show_col_types = FALSE), 
    error = function(e) return(NULL))
  
  # If file reading fails, skip processing
  if (is.null(analysis)) return(FALSE)
  
  # Normalize row names
  analysis[[1]] <- tolower(analysis[[1]])
  
  # Identify missing rows and add them, input NA value for second column
  missing_rows <- setdiff(expected_row_names, analysis[[1]])
  if (length(missing_rows) > 0) {
    missing_data <- tibble(!!colnames(analysis)[1] := missing_rows)
    for (i in 2:ncol(analysis)) {
      missing_data[[colnames(analysis)[i]]] <- NA
    }
    analysis <- bind_rows(analysis, missing_data)
  }
  
  # Reorder rows based on expected_row_names
  analysis <- arrange(analysis, match(analysis[[1]], expected_row_names))
  
  # Save the updated file as headerless
  write_csv(analysis, paste0(updated_data_dir, basename(file_path)), col_names = FALSE)
 
  return(TRUE)
}
```

We now apply this function to all data files. This process will generate an `updated_data` folder containing the cleaned files and a log file in the main directory.

```{r}
#| eval: false

# Process CSV files in the raw data directory and validate them
validation_results <- dir_ls(path = raw_data_dir, glob = "*.csv") %>%
  map_dfr(check_files)  # Apply check_files to each file and collect results

# Log the validation summary
summary_message <- sprintf(
  "Validation Summary:\nTotal files checked: %d\nTotal updated files: %d\n", 
  length(validation_results), 
  sum(validation_results)
)
write_lines(summary_message, log_file)  # Write summary to the log file
message(summary_message)                # Print summary to the console
```

The log file, now streamlined to include only this summary, initially contained detailed row name differences between the files and the SOP, based on a subset of 600 files.

This code can be run both locally and on an HPC with minimal adjustments, such as loading individual `dplyr` and `readr` packages instead of the entire `tidyverse`.

To check the differences between the raw and cleaned files (quick visual check to see if we are doing something).

```{bash}
#| eval: false
diff -yr --suppress-common-line /path/to/raw_directory /path/to/cleaned_directory
```

```{r}
# Remove unused values to unclutter the environment
rm(raw_data_dir, validation_results, summary_message)
```

### Combining the Raw Data in a Table

We made sure all analysis files have the same field names: `analysis_id`, `gene_accession_id`, `gene_symbol`, `mouse_strain`, `mouse_life_stage`, `parameter_id`, `parameter_name`, and `pvalue`.

To make data cleaning and processing easier, these files will be combined into one table using these shared fields. This way, we can check all values for a specific field at once (vectorized), making the process faster and simpler.

The `combine_files` function reads a file and transforms its data into a row for the unified table:

```{r}
combine_files <- function(file_path) {
  read_csv(file_path, show_col_types = FALSE) %>%  # Read the CSV file 
    t() %>%                                        # Transpose so fields become columns and values become rows
    as.data.frame() %>%                            # Convert the transposed data to a data frame
    setNames(.[1, ]) %>%                           # Set analysis_id (first column) as column name 
    slice(-1) %>%                                  # Remove the field names (first row)
    mutate(analysis_id = rownames(.), .before = 1) # Move analysis_id as the first column
}
```

The `combine_files` function is then applied to each file in the directory, combining their contents into a single table:

```{r}
#| eval: false

# Combine all data files into a single table
analysis <- dir_ls(path = updated_data_dir, glob = "*.csv") %>%  # List all CSV files in the directory
  map_dfr(combine_files) # Apply the combine_files function to each file and combine results

# Assign expected column names to the table
colnames(analysis) <- expected_row_names

write_csv(analysis, paste0(rootDir, "data/raw_data/analysis_table.txt")) # Save the combined table as a CSV
```

### QC and Cleaning the Combined Analysis Data

For each column we check whether column values are the right lengths (for strings) or size (for floats), and whether the results match allowed values, like for `mouse_strain` and `mouse_life_stage`. We also to checked if certain variables are perfectly correlated with one another.

First we load the data and generate a log file.

```{r}
# Read the collated analysis file
analysis <- read_csv(paste0(rootDir, "data/raw_data/analysis_table.txt"), show_col_types = FALSE)

# Define a custom logging function
log_message <- function(message) {
  cat(message, "\n", file = log_file, append = TRUE)  # Write message to log file
  message(message)                                    # Display message in the console
}

# Log the start of the process
log_message("=== Data Validation and Transformation Log ===")
```

Check if variable data types match those specified in the SOP - all character (or string) data types except for `pvalue` (numeric or float).

```{r}
# Extract expected data types from the SOP and map them to R-compatible types
expected_data_types <- read_csv(SOP_path, show_col_types = FALSE)$dataType %>% 
  recode("String" = "character", "Float" = "numeric")  # Map SOP data types to R data types

# Determine the actual data types of the analysis table columns
data_types <- sapply(analysis, class)

# Identify mismatched data types by comparing actual vs. expected
type_mismatches <- tibble(
  variable = names(data_types),                          # Column names
  actual_type = unname(data_types),                      # Actual data types
  expected_type = expected_data_types[names(data_types)] # Expected data types from SOP
) %>%
  filter(actual_type != expected_type)                   # Filter for mismatches

# Log the results of the type validation
if (nrow(type_mismatches) > 0) {
  log_message("Data type mismatches found:")                     # Log message header
  log_message(capture.output(print(type_mismatches)))            # Log mismatched types
} else {
  log_message("All data types match the SOP specification.")     # Log success message
}

# Cleanup temporary variables
rm(data_types, expected_data_types)
```

Check if `analysis_id` values are all 15 characters long and all unique.

```{r}
# Validate 'analysis_id' by checking length and uniqueness
invalid_ids <- analysis %>%
  filter(nchar(analysis_id) != 15 | duplicated(analysis_id))  # Identify invalid IDs

# Log the results of the validation
log_message(if (nrow(invalid_ids) == 0) {
  "All analysis_id values are valid."                          # Log success if no issues
} else {
  paste("Invalid analysis_id values found:", nrow(invalid_ids)) # Log count of invalid IDs
})
```

Check if `gene_accession_id` values are all between 9-11 characters long.

```{r}
# Validate 'gene_accession_id' by checking its length
invalid_gene_lengths <- analysis %>%
  filter(nchar(gene_accession_id) < 9 | nchar(gene_accession_id) > 11)  # Identify invalid lengths

# Log the results of the validation
log_message(if (nrow(invalid_gene_lengths) == 0) {
  "All gene_accession_id values are valid."                            # Log success if no issues
} else {
  paste("Invalid gene_accession_id lengths:", nrow(invalid_gene_lengths))  # Log count of invalid IDs
})
```

Check & format `gene_symbol` so all values are in title format.

```{r}
# Validate and format 'gene_symbol'

# Check for invalid gene_symbol lengths
invalid_gene_symbols <- analysis %>%
  filter(nchar(gene_symbol) < 1 | nchar(gene_symbol) > 13)  # Identify invalid lengths

# Log the results of the validation
log_message(if (nrow(invalid_gene_symbols) == 0) {
  "All gene_symbol values are valid."                            # Log success if no issues
} else {
  paste("Invalid gene_symbol values:", nrow(invalid_gene_symbols))  # Log count of invalid symbols
})
```

```{r}
# Convert all gene_symbol values to title case and remove duplicates
analysis <- analysis %>%
  mutate(gene_symbol = str_to_title(str_to_lower(gene_symbol))) %>%
  distinct()
```

Check if `gene_accession_id` and `gene_symbol` are perfectly correlated.

```{r}
# Check correlation between `gene_accession_id` and `gene_symbol`
gene_symbol_correlation <- analysis %>%
  select(gene_accession_id, gene_symbol) %>%
  distinct() %>%
  group_by(gene_accession_id) %>%
  filter(n_distinct(gene_symbol) > 1)  # Identify cases where one gene_accession_id maps to multiple gene_symbols

# Log the results of the correlation check
log_message(if (nrow(gene_symbol_correlation) == 0) {
  "Perfect correlation between gene_accession_id and gene_symbol."  # Log success if no issues
} else {
  "Correlation issues detected."  # Log message if issues are found
})
```

Check & format `mouse_strain` so all results are one of the four valid strains specified in the SOP: C57BL, B6J, C3H or 129SV. Our results show 11 unique strains, 9 of which appear to be typos of C57BL (e.g. C53BL, C58BL - none of the 'typos' are valid mouse strains according to MGI).

```{r}
unique(analysis$mouse_strain)
# Validate and fix `mouse_strain` by standardizing known values and replacing others with "C57BL"
analysis <- analysis %>%
  mutate(mouse_strain = ifelse(mouse_strain %in% c("C57BL", "B6J", "C3H", "129SV"), mouse_strain, "C57BL"))

# Log the update to mouse_strain values
log_message("Updated mouse_strain values. Non-standard values were replaced with 'C57BL' due to observed typos.")
```

Check if `mouse_life_stage` values are one of the allowed values according to the SOP.

```{r}
# Validate `mouse_life_stage` by checking against valid values
invalid_life_stages <- analysis %>%
  filter(!mouse_life_stage %in% c("E12.5", "E15.5", "E18.5", "E9.5", "Early adult", "Late adult", "Middle aged adult"))

# Log the results of the validation
log_message(if (nrow(invalid_life_stages) == 0) {
  "All mouse_life_stage values are valid."  # Log success if no issues
} else {
  paste("Invalid mouse_life_stage values:", nrow(invalid_life_stages))  # Log count of invalid values
})
```

Checking if `parameter_id` & `parameter_name` are the correct lengths (15-18 characters & 2-74 characters respectively). Also checking if they are perfectly correlated with each other.

```{r}
# Validate 'parameter_id' by checking its length
invalid_parameter_ids <- analysis %>%
  filter(nchar(parameter_id) < 15 | nchar(parameter_id) > 18)  # Identify invalid lengths

log_message(if (nrow(invalid_parameter_ids) == 0) {
  "All parameter_id values are valid."  # Log success if no issues
} else {
  paste("Invalid parameter_id values:", nrow(invalid_parameter_ids))  # Log count of invalid IDs
})

# Validate 'parameter_name' by checking its length
invalid_parameter_names <- analysis %>%
  filter(nchar(parameter_name) < 2 | nchar(parameter_name) > 74)  # Identify invalid lengths

log_message(if (nrow(invalid_parameter_names) == 0) {
  "All parameter_name values have valid lengths."  # Log success if no issues
} else {
  paste("Invalid parameter_name values:", nrow(invalid_parameter_names))  # Log count of invalid names
})

# Check correlation between 'parameter_id' and 'parameter_name'
parameter_correlation <- analysis %>%
  group_by(parameter_id) %>%
  summarise(unique_names = n_distinct(parameter_name)) %>%
  filter(unique_names > 1)  # Identify cases where one parameter_id maps to multiple parameter_names

log_message(if (nrow(parameter_correlation) == 0) {
  "Perfect correlation between parameter_id and parameter_name."  # Log success if no issues
} else {
  "Correlation issues detected for parameter_id and parameter_name."  # Log message if issues are found
})
```

Check if all `pvalues` are between 0 & 1, if not cap the values at those extremes. In this case, we had no values below 0 but we did have values above 1. All `pvalue` results are standardized to 6 decimal points.

```{r}
# Validate and fix 'pvalue'

# Cap p-values to be >=1 & round all results to 6 decimal points
analysis <- analysis %>%
  mutate(pvalue = ifelse(pvalue > 1, 1, round(pvalue, 6)))

# Identify invalid pvalues (out of range or missing)
invalid_pvalues <- analysis %>%
  filter(pvalue < 0 | pvalue > 1 | is.na(pvalue))

# Log the validation results
log_message(if (nrow(invalid_pvalues) == 0) {
  "All pvalue values are valid & standardized to 6 decimal points."  # Log success if no issues
} else {
  paste("Invalid pvalue values:", nrow(invalid_pvalues))  # Log count of invalid values
})
```

Finally, check the data-frame for rows with identical results in every column (excluding `analysis_id`, which are all identical). Once identified, these rows are removed.

```{r}
#
rows_before <- nrow(analysis)

### Check & remove duplicate rows (excluding 'analysis_id')
analysis <- analysis %>%
  distinct(across(-analysis_id), .keep_all = TRUE)

rows_after <- nrow(analysis)

log_message(paste(rows_before - rows_after, "duplicated rows removed. Updated dataset has",rows_after,"rows."))

```

Save the cleaned data-frame to a new file.

```{r}
# Save the cleaned analysis dataset to the cleaned data directory
write_csv(analysis, paste0(rootDir, "data/cleaned/cleaned_analysis_table.txt"))

# Log the successful save operation
log_message("Cleaned dataset saved as 'cleaned_analysis_table.txt'.")
log_message("=== End of Log ===")
```

Log file shows the all the messaged corresponding to the if/else statement results in every step of this workflow.

```{r}
# Remove temporary variables and objects to free up memory
rm(
  invalid_gene_lengths, invalid_gene_symbols, invalid_ids, invalid_life_stages,
  invalid_parameter_ids, invalid_parameter_names, invalid_pvalues, 
  parameter_correlation, type_mismatches, gene_symbol_correlation, 
  log_file, log_message, updated_data_dir, SOP_path, 
  rows_before, rows_after
)
```

## Cleaning the Metadata Files

Extra information about the parameters tested, the procedures they are part of, and the human diseases linked to the knockout genes is stored in separate metadata files. However, these files are not set up correctly for loading into R and have inconsistent values, so they need cleaning.

The cleaning process for the procedure, parameter, and disease files had two steps:

1.  **Initial Setup**: Fields were separated with commas, and any fields with commas inside were enclosed in quotes to make sure the file could load properly as a table. A temporary text file with these fixes was saved.

2.  **Data Cleaning**: The temporary file was loaded as a table, duplicates were removed, and formatting was made consistent.

Finally, a quick check was done to confirm that the fields followed the SOP rules.

### `IMPC_procedure.txt`

#### Formatting

-   Replaced double quotes between fields with commas.

-   Removed leading and trailing double quotes from each line.

-   Enclosed descriptions containing commas in quotes using a regular expression.

```{r}
# Define the path to temporarily store the formatted table before cleaning
temporary_path <- paste0(rootDir, "data/cleaned/IMPC_procedure_table.txt")

# Reformat the procedures file for proper table loading
procedure <- read_lines(paste0(rootDir, "data/metadata/IMPC_procedure.txt")) %>%
  str_replace_all('" "', ",") %>%           # Replace double quotes between fields with commas
  str_remove_all('^"|"$') %>%               # Remove leading and trailing double quotes from each line
  str_replace(                              # Wrap descriptions containing commas in quotes
    '^([0-9]+,[^,]+,)(.*)(, (TRUE|FALSE), [0-9]{5})$', 
    '\\1"\\2"\\3'
  ) %>%
  str_remove('line_number,') %>%            # Remove the "line_number" field name
  write_lines(temporary_path)               # Save the reformatted lines to the temporary file
```

#### Cleaning

-   Trimmed whitespace and replaced HTML codes.

-   Renamed columns for consistency.

-   Standardized or filled missing/conflicting descriptions based on procedure names.

-   Grouped identical procedures, assigned shared `procedure_id`s, and stored parameter IDs for integration with the parameter table.

```{r}
# Clean and standardize the procedure information
procedure <- read_csv(temporary_path, show_col_types = FALSE, col_names = TRUE) %>%
  
  # Apply basic cleaning steps
  mutate(across(everything(), ~ .x %>%
      str_trim() %>%                   # Trim leading and trailing whitespace
      str_replace_all(c(               # Replace HTML entity codes
        "&amp;" = "&",                 
        "&nbsp;" = " ")) %>%           
      na_if("")                        # Replace empty strings with NA
  )) %>% 
  
  # Rename columns for consistency throughout 
  rename(
    procedure_id = procedureId,                       
    procedure_name = name,
    procedure_description = description,
    is_mandatory = isMandatory,
    parameter_mapping = impcParameterOrigId
  ) %>% 
  mutate(
    # Convert mapping to numeric, introduces NA for invalid entries  
    parameter_mapping = as.numeric(parameter_mapping),
    # Convert the column to logical
    is_mandatory = as.integer(as.logical(is_mandatory)), 
  # Standardize conflicting descriptions for rows referring to the same observation
    procedure_description = case_when(
      procedure_name == "Experimental design" ~ "experimental_design",        
      procedure_name == "Housing and Husbandry" ~ "housing_and_husbandry",
      procedure_name == "Viability Primary Screen" ~ "Assess the viability of mutant mice of each sex and zygosity.",
      procedure_name == "Electrocardiogram (ECG)" ~ "To provide a high throughput method to obtain Electrocardiograms in a conscious mouse.",
      is.na(procedure_description) ~ tolower(str_replace_all(procedure_name, " ", "_")), # Fill missing descriptions
      TRUE ~ procedure_description
    )
  ) %>% 
  
  # Group procedures to assign unique procedure IDs
  group_by(procedure_name, is_mandatory, procedure_description) %>%
  summarise(
    procedure_id = cur_group_id(),               
    parameter_mapping = list(parameter_mapping),# Store original IDs as a list for mapping with parameters
    .groups = "drop"                             
  ) %>% 

  # Expand the list of parameter mappings into individual rows
 unnest(parameter_mapping)

# Add a new row to the cleaned procedure table
procedure <- procedure %>%
  add_row(
    procedure_id = max(procedure$procedure_id, na.rm = TRUE) + 1, # Assign a new unique ID
    procedure_name = "Unknown Procedure",                        # Specify the new procedure name
    procedure_description = "No information available for the procedure associated with this parameter", # Provide a description
    is_mandatory = FALSE,                                         # Indicate if it's mandatory
    parameter_mapping = NA                                        # Include NA for parameter_mapping if none
  )


# Remove formatted file 
file.remove(temporary_path)

# Save the cleaned data to a new CSV file
write_csv(procedure, paste0(rootDir, "data/cleaned/cleaned_IMPC_procedure.csv"))
```

The cleaned table reduced over 5,000 rows to [50 distinct procedures](https://www.mousephenotype.org/impress/PipelineInfo?id=7) and ensured all fields met SOP compliance.

#### Validating

```{r}
# Validate procedure table for SOP compliance
errors <- c()

# Ensure procedure_name and procedure_description are strings
if (!all(sapply(procedure$procedure_name, is.character))) {
  errors <- c(errors, "'procedure_name' contains non-string values.")
}
if (!all(sapply(procedure$procedure_description, is.character))) {
  errors <- c(errors, "'procedure_description' contains non-string values.")
}

# Ensure is_mandatory contains only TRUE or FALSE
if (!all(procedure$is_mandatory %in% c(TRUE, FALSE))) {
  errors <- c(errors, "'is_mandatory' contains invalid values (not TRUE/FALSE).")
}

# Ensure procedure_id is a unique numeric value
if (!all(is.numeric(procedure$procedure_id), na.rm = TRUE)) {
  errors <- c(errors, "'procedure_id' contains non-numeric values.")
}
# Note: this only works when the parameter_mapping field is nested
if (length(unique(procedure$procedure_id)) != nrow(procedure)) {
  errors <- c(errors, "'procedure_id' contains duplicate values. This is normal if the parameter_mapping field is unnested.")
}

# Ensure parameter_mapping is numeric
if (!all(is.numeric(procedure$parameter_mapping), na.rm = TRUE)) {
  errors <- c(errors, "'parameter_mapping' contains non-numeric values. This is normal if the parameter_mapping field is nested.")
}

# Output validation results
if (length(errors) == 0) {
  print("All checks passed successfully.")
} else {
  print(paste("Errors found:", paste(errors, collapse = " ")))
}
```

### `IMPC_parameter_description.txt`

#### Formatting

-   Removed the header line and empty lines.

-   Stripped leading and trailing double quotes.

-   Replaced double quotes between fields with commas.

-   Replaced inconsistent newline characters with spaces.

-   Corrected field separators and properly quoted descriptions using a regular expression.

```{r}
# Define the path to temporarily store the formatted table before cleaning
temporary_path <- paste0(rootDir, "data/cleaned/IMPC_parameter_description_table.txt")


# Reformat the parameters file for proper table loading
parameter <- read_lines(paste0(rootDir, "data/metadata/IMPC_parameter_description.txt")) %>%
  
  .[-1] %>%                                   # Remove the first line (header 'x')
  discard(~ . == "") %>%                      # Remove empty lines
  str_remove_all('^"|"$') %>%                 # Remove leading and trailing double quotes
  str_replace_all('" "', ",") %>%             # Replace double quotes separating fields with commas
  
  # Handle inconsistent newline characters
  str_c(collapse = "\n") %>%                  # Combine all lines into a single string
  str_replace_all("\n", " ") %>%              # Replace all existing newline characters with spaces
  # Add newline characters at the correct locations
  str_replace_all(c(
    '(IMPC_[A-Z]{3}_[0-9]{3}_[0-9]{3})' = '\\1\n',  # Add a newline after each gene identifier (single observation)
    'line_number,impcParameterOrigId, name, description, parameterId' = 
      'parameter_mapping, parameter_name, parameter_description, parameter_id\n'  # Standardize header row and add a newline
  )) %>%
  str_split("\n") %>%                         # Split the string into lines based on the newline character
  unlist() %>%                                # Convert the resulting list into a flat character vector
  
  str_trim() %>%                              # Trim whitespace from the start and end of each line
  
  # Replace commas within parentheses with semicolons to avoid misinterpretation as field separators
  str_replace_all('\\(([^)]*)\\)', function(x) str_replace_all(x, ',', ';')) %>%
  
  # Format table rows by adding quotes around fields
  str_replace_all(
    '^([1-9][0-9]{0,3}),([0-9]{4,5}), ?([^,]+), ?(.*), ?(IMPC_[A-Z]{3}_[0-9]{3}_[0-9]{3})$', 
    '"\\2","\\3","\\4","\\5"'
  ) %>%
  
  # Replace semicolons within parentheses back to commas
  str_replace_all('\\(([^)]*)\\)', function(x) str_replace_all(x, ';', ',')) %>%
  
  # Save the cleaned and standardized data to a new file
  write_lines(temporary_path)
```

#### Cleaning

-   Trimmed whitespace and replaced empty strings with `NA`.

-   Renamed columns for consistency.

-   Standardized parameter names and descriptions using `parameter_id` and the IMPC database, filling missing descriptions with standardized names.

-   Removed duplicates to ensure one record per parameter, reducing rows from 5,325 to 3,600.

```{r}
# Load and clean parameter data
parameter <- read_csv(temporary_path, show_col_types = FALSE) %>%
  
  # Clean all table cells: trim whitespace and replace empty strings with NA
  mutate(across(everything(), ~ .x %>%
      str_trim() %>%   
      na_if("")        
  )) %>%  
  mutate(
    # Convert mapping to numeric, introduces NA for invalid entries  
    parameter_mapping = as.numeric(parameter_mapping),  
    # Update inconsistent parameter names
    parameter_name = case_when(
      # Experimental design procedure
      parameter_id == "IMPC_EXD_007_001" ~ "Frequency of controls",
      parameter_id == "IMPC_EXD_008_001" ~ "Number male controls",
      parameter_id == "IMPC_EXD_009_001" ~ "Number female controls",
      parameter_id == "IMPC_EXD_014_001" ~ "Core stock strategy",
      parameter_id == "IMPC_EXD_015_001" ~ "Knockout batch strategy",
      # Histopathology procedure
      parameter_id == "IMPC_HIS_008_001" ~ "Eye - Free text diagnostic term",
      parameter_id == "IMPC_HIS_010_001" ~ "Eye - Severity score",
      parameter_id == "IMPC_HIS_011_001" ~ "Eye - Descriptor PATO",
      parameter_id == "IMPC_HIS_012_001" ~ "Eye - Description",
      parameter_id == "IMPC_HIS_140_001" ~ "Testis - Free text diagnostic term",
      parameter_id == "IMPC_HIS_142_001" ~ "Testis - Severity score",
      parameter_id == "IMPC_HIS_143_001" ~ "Testis - Descriptor PATO",
      parameter_id == "IMPC_HIS_144_001" ~ "Testis - Description",
      parameter_id == "IMPC_HIS_152_001" ~ "Prostate gland - Free text diagnostic term",
      parameter_id == "IMPC_HIS_154_001" ~ "Prostate gland - Severity score",
      parameter_id == "IMPC_HIS_155_001" ~ "Prostate gland - Descriptor PATO",
      parameter_id == "IMPC_HIS_156_001" ~ "Prostate gland - Description",
      parameter_id == "IMPC_HIS_180_002" ~ "Eye - Significance score",
      parameter_id == "IMPC_HIS_202_002" ~ "Testis - Significance score",
      parameter_id == "IMPC_HIS_204_002" ~ "Prostate gland - Significance score",
      parameter_id == "IMPC_HIS_259_001" ~ "Trigeminal ganglion - Diagnostic term",
      parameter_id == "IMPC_HIS_261_001" ~ "Trigeminal ganglion - Severity score",
      parameter_id == "IMPC_HIS_262_001" ~ "Trigeminal ganglion - Descriptor PATO",
      parameter_id == "IMPC_HIS_263_001" ~ "Trigeminal ganglion - Description",
      parameter_id == "IMPC_HIS_264_001" ~ "Trigeminal ganglion - Significance score",
      # Housing and Husbandry procedure
  parameter_id == "IMPC_HOU_042_001" ~ "Nutrition - Diet Mass Known", # Nutrition - do you know the composition of the diet (average based on mass)

  parameter_id == "IMPC_HOU_046_001" ~ "Nutrition - Diet Cal Known", # Nutrition - do you know the composition of the diet (average based on calorific content)
  parameter_id == "IMPC_HOU_047_001" ~ "Nutrition - Diet Cal Carb", # Nutrition - average composition (based on calorific content) - Carbohydrate
  parameter_id == "IMPC_HOU_064_001" ~ "Microbiological Status - Pathogen Positive", # Microbiological status - is your unit positive for any of the pathogens tested
      TRUE ~ parameter_name  # Default case
    ),
    # Fill or update parameter descriptions
    parameter_description = case_when(
      parameter_id == "IMPC_EXD_015_001" ~ "Knockout batch strategy",
      parameter_id == "IMPC_HIS_008_001" ~ "Eye - Free text diagnostic term",
      parameter_id == "IMPC_HIS_010_001" ~ "Eye - Severity score",
      is.na(parameter_description) ~ tolower(str_replace_all(parameter_name, " ", "_")), # Fill missing descriptions with underscored name 
      TRUE ~ parameter_description  # Default case
    )
  ) %>%
  
  # Ensure one record per parameter
  distinct(parameter_name, parameter_description, parameter_id, .keep_all = TRUE)  # Reduces rows from 5325 to 3600; ignore parameter_mappings for this as each parameter is part of a specific procedure 

# Remove formatted file 
file.remove(temporary_path)

# Save the cleaned data to a new CSV file
write_csv(parameter, paste0(rootDir, "data/cleaned/cleaned_IMPC_parameter_description.csv"))
```

#### Validating

```{r}
# Check the parameter table for SOP compliance
errors <- c()

# Check if parameter_mapping is numeric
if (!all(is.numeric(parameter$parameter_mapping), na.rm = TRUE)) {
  errors <- c(errors, "Some 'parameter_mapping' values are not numeric.")
}

# Check if parameter_name, parameter_description, and parameter_id are strings
if (!all(sapply(parameter$parameter_name, is.character))) {
  errors <- c(errors, "Some 'parameter_name' values are not strings.")
}
if (!all(sapply(parameter$parameter_description, is.character))) {
  errors <- c(errors, "Some 'parameter_description' values are not strings.")
}
if (!all(sapply(parameter$parameter_id, is.character))) {
  errors <- c(errors, "Some 'parameter_id' values are not strings.")
}

# Check if parameter_id are unique
if (length(unique(parameter$parameter_id)) != nrow(parameter)) {
  errors <- c(errors, "Duplicate 'parameter_id' values found.")
}

# Check if parameter_id and parameter_name length are within SOP constraints 
if (!all(nchar(parameter$parameter_id) >= 15 & nchar(parameter$parameter_id) <= 18, na.rm = TRUE)) {
  errors <- c(errors, "Some 'parameter_id' lengths are outside SOP constraints (15-18 characters).")
}
if (!all(nchar(parameter$parameter_name) >= 2 & nchar(parameter$parameter_name) <= 74, na.rm = TRUE)) {
  errors <- c(errors, "Some 'parameter_name' lengths are outside SOP constraints (2-74 characters).")
}

# Print results
if (length(errors) == 0) {
  print("All checks passed successfully.")
} else {
  print(paste("Errors found:", paste(errors, collapse = " ")))
}
```

### `disease_information.txt`

#### Formatting

-   Removed the header line and line numbers.

-   Stripped leading and trailing double quotes.

-   Used a regular expression to separate fields, ensuring proper quoting and field separation.

```{r}
temporary_path <- paste0(rootDir, "data/cleaned/Disease_information_table.txt")
  
disease <- read_lines(paste0(rootDir, "data/metadata/Disease_information.txt")) %>% 
  .[-1] %>%    # Remove the first line (header 'x')
  str_remove_all('^\\"\\d+\\"\\s') %>% # Remove  line numbers enclosed in quotes followed by a space
  str_remove_all('^"|"$') %>%     # Remove any leading or trailing quotes
  
  # Separate fields based on the regex 
  str_replace_all(
    '(OMIM|ORPHA|DECIPHER):(\\d+),\\s*(.+?),\\s*(MGI:\\d+),\\s*(\\d+)',
    '"\\1:\\2","\\3","\\4","\\5"') %>%

  # Save the cleaned and standardized data to a new file
  write_lines(temporary_path)
```

#### Cleaning

-   Trimmed white space and replaced empty strings with `NA`.

-   Converted `phenodigm_score` to numeric format.

-   Removed duplicate rows.

##### Identified rows with the same `disease_id` and `gene_accession_id` for further discussion. {style="color: red"}

```{r}
# Load and clean parameter data
disease <- read_csv(temporary_path, show_col_types = FALSE) %>%
  mutate(across(everything(), ~ .x %>%
      str_trim() %>%   
      na_if("")        
  )) %>%  
  mutate(
    # Convert safely to numeric
    phenodigm_score = as.numeric(phenodigm_score)
  )%>%
  # Remove duplicates
  distinct()

# Remove formatted file 
file.remove(temporary_path)

# Save the cleaned data to a new CSV file
write_csv(disease, paste0(rootDir, "data/cleaned/cleaned_disease_information.csv"))

## WE STILL NEED TO DISCUSS HOW WE WANT TO ADRESS THIS 
# Find rows with the same disease_id and gene_accession_id
duplicates <- disease %>%
  group_by(disease_id, gene_accession_id) %>%
  filter(n() > 1) %>%
  ungroup()

# Check if duplicates exist and display them
if (nrow(duplicates) > 0) {
  cat("Rows with the same disease_id and gene_accession_id:\n")
  print(duplicates)
} else {
  cat("No rows with duplicate disease_id and gene_accession_id found.\n")
}

```

#### Validating

```{r}
# Validate disease table for SOP compliance
errors <- c()

# Ensure disease_id, disease_term, and gene_accession_id are strings
if (!all(sapply(disease$disease_id, is.character))) {
  errors <- c(errors, "'disease_id' contains non-string values.")
}
if (!all(sapply(disease$disease_term, is.character))) {
  errors <- c(errors, "'disease_term' contains non-string values.")
}
if (!all(sapply(disease$gene_accession_id, is.character))) {
  errors <- c(errors, "'gene_accession_id' contains non-string values.")
}

# Ensure phenodigm_score is a double
if (!all(is.double(disease$phenodigm_score), na.rm = TRUE)) {
  errors <- c(errors, "'phenodigm_score' contains non-double values.")
}

# Validate disease_id format (starts with ORPHA, OMIM, or DECIPHER)
if (!all(grepl("^(ORPHA|OMIM|DECIPHER):\\d+$", disease$disease_id))) {
  errors <- c(errors, "'disease_id' does not conform to the expected format (ORPHA, OMIM, or DECIPHER followed by a colon and digits).")
}

# Validate gene_accession_id format (starts with MGI and is 9 to 11 characters)
if (!all(grepl("^MGI:\\d{5,7}$", disease$gene_accession_id))) {
  errors <- c(errors, "'gene_accession_id' does not conform to the expected format (MGI: followed by 5 to 7 digits).")
}

# Output validation results
if (length(errors) == 0) {
  print("All checks passed successfully.")
} else {
  print(paste("Errors found:", paste(errors, collapse = " ")))
}
```

```{r}
rm(errors, temporary_path)
```

# Collating

![](images/Screenshot%202024-12-27%20at%2019.24.12.png)

Although the metadata and analysis tables have been cleaned, they do not match the agreed database structure. To make them compatible, new tables are created by selecting and transforming specific fields from the cleaned data. Some tables require merging information from multiple sources. The final outputs are saved as CSV files for upload into the database.

To avoid re-running the entire script, the cleaned tables are loaded:

```{r}
procedure <- read_csv(paste0(rootDir, "data/cleaned/cleaned_IMPC_procedure.csv"), show_col_types = FALSE) 
analysis <- read_csv(paste0(rootDir, "data/cleaned/cleaned_analysis_table.txt"), show_col_types = FALSE) 
parameter <- read_csv(paste0(rootDir, "data/cleaned/cleaned_IMPC_parameter_description.csv"), show_col_types = FALSE)
disease <- read_csv(paste0(rootDir, "data/cleaned/cleaned_Disease_information.csv"), show_col_types = FALSE)
```

## **Procedures Table**

Key fields (`procedure_id`, `procedure_name`, `procedure_description`, `is_mandatory`) are selected, with `is_mandatory` converted to integers (0 or 1) for SQL compatibility.

```{r}
procedures_table <- procedure %>%
  select(procedure_id, procedure_name, procedure_description, is_mandatory) %>%
  distinct()

write_csv(procedures_table, paste0(rootDir, "/data/collated/Procedures.csv"), quote = "all")
```

## **Genes Table**

Gene information (`gene_accession_id`, `gene_symbol`) is combined from the analysis and disease tables. Missing symbols are curated from the MGI database for about a dozen genes.

```{r}
genes_table <- analysis %>%
  select(gene_accession_id, gene_symbol) %>%
  bind_rows(
    disease %>%
      select(gene_accession_id) %>%
      filter(!gene_accession_id %in% analysis$gene_accession_id)
  ) %>%
  mutate(
    gene_symbol = case_when(
      gene_accession_id == "MGI:98967" ~ "Wt",
      gene_accession_id == "MGI:96090" ~ "Asmt",
      gene_accession_id == "MGI:98266" ~ "Sord",
      gene_accession_id == "MGI:1928271" ~ "Malrd1",
      gene_accession_id == "MGI:109279" ~ "Nnt",
      gene_accession_id == "MGI:3665157" ~ "Rsxr",
      gene_accession_id == "MGI:1345961" ~ "Coro1a",
      gene_accession_id == "MGI:87868" ~ "Acads",
      gene_accession_id == "MGI:1351634" ~ "Abcc6",
      gene_accession_id == "MGI:104511" ~ "Tnfsf4",
      TRUE ~ gene_symbol
    )
  ) %>%
  distinct()

write_csv(genes_table, paste0(rootDir, "/data/collated/Genes.csv"), quote = "all")
```

## **Analyses Table**

Relevant fields are selected from the analysis table.

```{r}
analyses_table <- analysis %>%
  select(analysis_id, gene_accession_id, mouse_strain, mouse_life_stage, parameter_id, pvalue) %>%
  distinct()

write_csv(analyses_table, paste0(rootDir, "/data/collated/Analyses.csv"), quote = "all")
```

There are some gene-parameter combos that occur up to 12 times. If we filter for life stage and mouse strain, we get up to 4 replicates of a gene-parameter-stage-strain combo.

```{r}
## THIS SECTION IS EXPLORATORY! 
# Count the number of duplicates for each gene_accession_id and parameter_id combination
duplicate_counts <- analyses_table %>%
  group_by(gene_accession_id, parameter_id) %>%
  summarize(count = n()) %>%  # Count the occurrences
  filter(count > 1) %>%       # Keep only combinations with duplicates
  ungroup()                   # Remove grouping for further manipulation

# View the result
#print(duplicate_counts)

# Find the maximum count of duplicates
max_duplicate_count <- duplicate_counts %>%
  summarize(max_count = max(count)) %>%
  pull(max_count)

# Print the maximum count
#print(max_duplicate_count)


# Filter rows with the maximum count
rows_with_max_count <- duplicate_counts %>%
  filter(count == max_duplicate_count)

# View the rows
print(rows_with_max_count)

```

## **Parameters Table**

The parameter table is joined with the procedure table using `parameter_mapping`, and missing `parameter_id` values from the analyses table are added.

```{r}
parameters_table <- left_join(parameter, procedure, by = join_by(parameter_mapping)) %>%
  bind_rows(
    analyses_table %>%
      select(parameter_id) %>%
      filter(!parameter_id %in% parameter$parameter_id)
  ) %>%
  filter(!if_all(everything(), is.na)) %>%
  # Add the "unknown procedure" descriptor with parameters with no procedure information to meet the foreign key requirements 
  mutate(
    procedure_id = if_else(
      is.na(procedure_id), 
      max(procedure$procedure_id, na.rm = TRUE),
      procedure_id), # Replace missing parameter names with parameter ID to enable visualisation
      parameter_name = ifelse(is.na(parameter_name), parameter_id, parameter_name
    )) %>% 
  distinct(parameter_id, parameter_name, parameter_description, procedure_id)

write_csv(parameters_table, paste0(rootDir, "/data/collated/Parameters.csv"), quote = "all")
```

## **Diseases Table**

Disease information (`disease_id`, `disease_term`) is extracted.

```{r}
diseases_table <- disease %>%
  select(disease_id, disease_term) %>%
  distinct()

write_csv(diseases_table, paste0(rootDir, "/data/collated/Diseases.csv"), quote = "all")
```

## **Phenodigm Scores Table**

Disease and gene information along with `phenodigm_score` is selected from the disease table.

```{r}
phenodigm_scores <- disease %>%
  select(disease_id, gene_accession_id, phenodigm_score) %>%
  distinct()

write_csv(phenodigm_scores, paste0(rootDir, "/data/collated/PhenodigmScores.csv"), quote = "all")
```

## Parameter Groupings

The goal is to group parameters by naming or phenotype test similarity, reducing parameter space and organizing them into meaningful categories (e.g., weight, images, brain). At least three new parameter groups are needed beyond the initial ones.

### Define Group Patterns

The `group_patterns` table defines group names (e.g., "weight," "images," "brain") with regex patterns to match parameter names, forming the basis for identifying and categorizing parameters.

```{r}
group_patterns <- tibble(
  group_name = c(
    "weight", "images", "brain", "cardiovascular", "eye", 
    "metabolic", "morphology", "audiology", 
    "immunology", "Behavior"
  ),
  regex_pattern = c(
    "weight|density|fat|lean|BMI|BMC|mass|composition|bone|length|adipose|lipid|body size|size|growth|skeletal|area|volume",
    "image|screenshot|microscope|pdf|lacz|waveform|ogram|resolution|visual|pixel|scan|photo|imaging|bit depth|microscopy|contrast",
    "brain|cortex|hippo|thalamus|amygdala|lobe|cere|spinal|pituitary|striatum|hypothal|neural|neuronal|pons|medulla|ganglion|nerve|enceph|ventricle|choroid",
    "heart|cardiac|vascular|blood|vein|aort|arter|HR|stroke|ejection|QT|circulation|pulse|pressure|rate|systolic|diastolic|respiration|respiratory|PR|pulse|ECG|cardio|valve|LVPWs|platelet",
    "eye|retina|cornea|lens|eyelid|iris|optic|ocular|vision|acuity|photoreceptor|brightness|scheimpflug|nuclear|macula|pupil|color|refraction|sclera|vitreous|retinal|lens",
    "glucose|insulin|cholesterol|lipid|triglycerides|fasting|diet|energy|metabolism|metabolic|AUC|glycemic|resistance|glucagon|carbohydrate|diabetes|protein|fat|urea|sodium|potassium|phosphorus|calories|homeostasis|hdl|ldl",
    "morphology|shape|size|growth|length|hypoplasia|syndactyly|microcephaly|anatomy|limb|edema|facial|cleft|dysmorphic|developmental|pattern|CRL|development|blebs|appearance|structure|pattern|thickness|pelvis|hair|color|coat",
    "ABR|hearing|audiometry|waveform|tone|decibel|audiogram|stimulus|frequency|click|response|threshold|sound|ear",
    "Lymphocyte|Neutrophil|NKT|Treg|Leukocyte|Basophil|Platelet|Spleen|Blood|Cell|Immune|Differential|White|Hemoglobin|Eosinophil|Monocyte",
    "behavior|locomotor|percentage|movement|time|center|speed|distance|resting|stimuli|startle|unexpected|vocalization|entries|rears|tremor|tongue|curl|vibrissae|aggression|pallor|turning|activity"
  )
) %>%
  mutate(
    regex_pattern = str_to_lower(regex_pattern),       # Convert regex to lowercase
    group_name = str_to_title(group_name)             # Convert group name to title case
  )
```

### Create Groupings Table

The `groupings_table` assigns a unique ascending `group_id` to each group name from the `group_patterns` table, retaining only `group_id` and `group_name`.

```{r}
groupings_table <- group_patterns %>%
  mutate(group_id = row_number()) %>%  # Assign unique ascending IDs
  select(group_id, group_name) %>% 
  mutate(
    group_name = str_to_title(group_name)
  ) %>%      
  distinct()  # Retain only group_id and group_name

write_csv(groupings_table, paste0(rootDir, "data/collated/Groupings.csv"), quote = "all")
```

### Map Parameters to Groups

The `parameter` table is processed to match each `parameter_id` with groups using regex patterns from `group_patterns`. Matches are expanded into separate rows for multiple groups, joined with the `groupings_table` to add `group_id`, and duplicates are removed.

```{r}
groupXparameter_table <- parameter %>%
  select(parameter_id, parameter_name) %>%                # Select relevant columns
  mutate(group_name = map(parameter_name, ~group_patterns %>%
                            filter(str_detect(.x, regex(regex_pattern, ignore_case = TRUE))) %>%
                            pull(group_name)))%>%
  unnest_longer(group_name) %>%                           # Expand rows for each matched group
  left_join(groupings_table, by = "group_name") %>%       # Add group_id from groupings_table
  select(parameter_id, group_id) %>%                      # Retain parameter_id and group_id columns
  distinct()                                              # Remove duplicates

write_csv(groupXparameter_table, paste0(rootDir, "/data/collated/ParameterGroupings.csv"), quote = "all")
```

# MySQL Database

After creating the tables in R, the database can be set up in MySQL, and the tables can be imported directly using the `LOAD DATA LOCAL INFILE` function.

This script checks if MySQL is running and connects with local file import enabled:

```{sql}
#| eval: false
# Connect to MySQL with local file import enabled
mysql -u root -p --local-infile=1
```

## Create Database and Tables

The first step is to create the database in MySQL, followed by creating the tables according to the agreed-upon schema.

```{sql}
#| eval: false

-- Create the database
CREATE DATABASE IMPCDb;

-- Select the database
USE IMPCDb;
```

```{sql}
#| eval: false

-- Create tables without foreign keys first

CREATE TABLE Genes (
    gene_accession_id VARCHAR(11) PRIMARY KEY,       -- Unique ID for each gene (9-11 characters)
    gene_symbol VARCHAR(13)                          -- Gene symbol (1-13 characters)
) COMMENT = 'Stores gene identifiers and symbols.';

CREATE TABLE Procedures (
    procedure_id INT PRIMARY KEY,                    -- Unique ID for each procedure
    procedure_name VARCHAR(47),                      -- Procedure name (max 47 characters)
    procedure_description VARCHAR(1090),             -- Procedure description (max 1090 characters)
    is_mandatory BOOLEAN NOT NULL DEFAULT FALSE      -- Mandatory flag (default FALSE)
) COMMENT = 'Stores procedure details.';

CREATE TABLE Diseases (
    disease_id VARCHAR(50) PRIMARY KEY,              -- Unique ID for each disease (max 50 characters)
    disease_term VARCHAR(150)                        -- Descriptive term for the disease (max 150 characters)
) COMMENT = 'Stores disease identifiers and terms.';

CREATE TABLE Groupings (
    group_id INT AUTO_INCREMENT PRIMARY KEY,         -- Auto-incrementing unique ID for each group
    group_name VARCHAR(50)                           -- Group name (max 50 characters)
) COMMENT = 'Defines parameter groups.';

-- Create tables with foreign keys

CREATE TABLE Parameters (
    parameter_id VARCHAR(18) PRIMARY KEY,            -- Unique ID for each parameter (15-18 characters)
    parameter_name VARCHAR(74),                      -- Parameter name (2-74 characters)
    parameter_description VARCHAR(1000),             -- Parameter description (max 1000 characters)
    procedure_id INT,                                -- Foreign key to Procedures
    FOREIGN KEY (procedure_id) REFERENCES Procedures(procedure_id)
) COMMENT = 'Stores parameter metadata.';

CREATE TABLE Analyses (
    analysis_id VARCHAR(15) PRIMARY KEY,             -- Unique ID for each analysis (15 characters)
    gene_accession_id VARCHAR(11),                   -- Foreign key to Genes
    mouse_strain VARCHAR(5),                         -- Mouse strain (3-5 characters)
    mouse_life_stage VARCHAR(17),                    -- Mouse life stage (4-17 characters)
    parameter_id VARCHAR(18),                        -- Foreign key to Parameters
    p_value FLOAT,                                   -- P-value (0-1)
    FOREIGN KEY (gene_accession_id) REFERENCES Genes(gene_accession_id),
    FOREIGN KEY (parameter_id) REFERENCES Parameters(parameter_id)
) COMMENT = 'Links Genes and Parameters with analysis results.';

CREATE TABLE PhenodigmScores (
    phenodigm_id INT AUTO_INCREMENT PRIMARY KEY,     -- Auto-incrementing unique ID
    disease_id VARCHAR(50),                          -- Foreign key to Diseases
    gene_accession_id VARCHAR(11),                   -- Foreign key to Genes
    phenodigm_score FLOAT,                           -- Association score
    FOREIGN KEY (disease_id) REFERENCES Diseases(disease_id),
    FOREIGN KEY (gene_accession_id) REFERENCES Genes(gene_accession_id)
) COMMENT = 'Links Genes and Diseases with association scores.';

CREATE TABLE ParameterGroupings (
    parameter_id VARCHAR(18),                        -- Foreign key to Parameters
    group_id INT,                                    -- Foreign key to Groupings
    PRIMARY KEY (parameter_id, group_id),            -- Composite key
    FOREIGN KEY (parameter_id) REFERENCES Parameters(parameter_id),
    FOREIGN KEY (group_id) REFERENCES Groupings(group_id)
) COMMENT = 'Links Parameters and Groupings.';
```

##### Someone should review the constraints (character limits, mandatory fields, and foreign keys) to ensure they align with the data and the SOP. For fields not covered in the SOP, check the actual maximum lengths in the data to assign appropriate character limits. {style="color: red"}

## Populate Database

With the tables created and CSV files ready, data can be imported into MySQL using `LOAD DATA INFILE` with these specifications: fields separated by commas, optionally enclosed in double quotes, and skipping the header row.

```{sql}
#| eval: false

LOAD DATA LOCAL INFILE '/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/data/collated/Procedures.csv'
INTO TABLE Procedures
FIELDS TERMINATED BY ','    
OPTIONALLY ENCLOSED BY '"'    
IGNORE 1 LINES;             
# Records: 52  Deleted: 0  Skipped: 0  Warnings: 0

LOAD DATA LOCAL INFILE '/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/data/collated/Genes.csv'
INTO TABLE Genes
FIELDS TERMINATED BY ',' 
OPTIONALLY ENCLOSED BY '"' 
IGNORE 1 LINES;
# Records: 210  Deleted: 0  Skipped: 0  Warnings: 0


LOAD DATA LOCAL INFILE '/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/data/collated/Groupings.csv'
INTO TABLE Groupings
FIELDS TERMINATED BY ',' 
OPTIONALLY ENCLOSED BY '"' 
IGNORE 1 LINES;
# Records: 10  Deleted: 0  Skipped: 0  Warnings: 0

LOAD DATA LOCAL INFILE '/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/data/collated/Parameters.csv'
INTO TABLE Parameters
FIELDS TERMINATED BY ',' 
OPTIONALLY ENCLOSED BY '"' 
IGNORE 1 LINES;
# Records: 3743  Deleted: 0  Skipped: 0  Warnings: 0


LOAD DATA LOCAL INFILE '/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/data/collated/Diseases.csv'
INTO TABLE Diseases
FIELDS TERMINATED BY ',' 
OPTIONALLY ENCLOSED BY '"' 
IGNORE 1 LINES;
# Records: 1097  Deleted: 0  Skipped: 0  Warnings: 0

LOAD DATA LOCAL INFILE '/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/data/collated/PhenodigmScores.csv'
INTO TABLE PhenodigmScores
FIELDS TERMINATED BY ',' 
OPTIONALLY ENCLOSED BY '"' 
IGNORE 1 LINES
(disease_id, gene_accession_id, phenodigm_score); -- Explicitly map columns (phenodigm_id is auto-generated)
# Records: 1219  Deleted: 0  Skipped: 0  Warnings: 0

LOAD DATA LOCAL INFILE '/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/data/collated/Analyses.csv'
INTO TABLE Analyses
FIELDS TERMINATED BY ',' 
OPTIONALLY ENCLOSED BY '"' 
IGNORE 1 LINES;
# Records: 178176  Deleted: 0  Skipped: 0  Warnings: 0

LOAD DATA LOCAL INFILE '/Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/data/collated/ParameterGroupings.csv'
INTO TABLE ParameterGroupings
FIELDS TERMINATED BY ','               
OPTIONALLY ENCLOSED BY '"'                   
IGNORE 1 LINES;  
# Records: 2340  Deleted: 0  Skipped: 0  Warnings: 0
```

##### A total of 143 parameters corresponding to 83,712 analysis files have non-IMPC identifiers, meaning they are not linked to any procedure. This violates the foreign key constraints in the database, resulting in the following issues: {style="color: red"}

1.  During the loading of the `Parameters` table, missing or invalid `procedure_id` values (e.g., `NA`) cause errors, such as "Incorrect integer value: 'NA' for column 'procedure_id' at row 3601." Additionally, foreign key violations occur when `procedure_id` values are not linked to `Procedures`.

2.  During the loading of the `Analyses` table, non-IMPC `parameter_id` values prevent rows from referencing the `Parameters` table, resulting in foreign key constraint failures.

##### Currently, parameters and related analyses that lack a procedure are skipped during upload. To avoid this, we could assign such parameters to an 'unknown/undefined' procedure by adding a corresponding row to the procedure table, ensuring the data is loaded and associated with an unknown value instead of being omitted. {style="color: red"}

```{bash}
#| eval: false

# Create database dump 
/opt/homebrew/bin/mysqldump IMPCDb -u root -p > /Users/aliciahobdell/Desktop/data_cleaning/DCDM_group3/data/database/IMPCDb.dump
```

## Creating the database from the dump file

```{bash}
# Log into MySQL
mysql -u root -p
```

```{sql}
# Create empty database named IMPCDb
CREATE DATABASE IMPCDb;
EXIT;
```

```{bash}
# Import the dump file
mysql -u root -p IMPCDb < "/Users/ellajadealex/Desktop/applied_bioinformatics/DCDM/group_project/DCDM_IMPC_Project/data/database/IMPCDb.dump"
```

# R/Shiny

**Test R/Shiny 1:** They want the ability to select a particular knockout mouse and visualise the statistical scores of all phenotypes tested.

```{sql}
#| eval: false

SELECT Analyses.p_value, Parameters.parameter_name FROM Analyses 
JOIN Parameters ON Analyses.parameter_id = Parameters.parameter_id 
WHERE Analyses.gene_accession_id IN (
    SELECT gene_accession_id FROM Genes WHERE gene_symbol IN ('Ifi204', 'Mettl21c', 'Ier5', 'Abca4'))
ORDER BY Analyses.p_value ASC;
```

**Test R/Shiny 2:** The collaborator also wishes to visualise the statistical scores of all knockout mice for a selected phenotype.

```{sql}
#| eval: false
SELECT Analyses.p_value, Genes.gene_symbol FROM Analyses 
JOIN Parameters ON Analyses.parameter_id = Parameters.parameter_id 
JOIN Genes ON Analyses.gene_accession_id = Genes.gene_accession_id 
WHERE Parameters.parameter_name IN ("Fusion of vertebrae", "Hindbrain morphology", "Limb morphology") -- Random parameter names 
ORDER BY Analyses.p_value ASC;
```

**Test R/Shiny 3:** The collaborator would like to visualise clusters of genes with similar phenotype scores.¬†¬†

```{sql}
#| eval: false
-- This query retrieves the gene symbols, parameter names, and p-values from the `Analyses` table, combining data from the `Parameters` and `Genes` tables.
SELECT Genes.gene_symbol, Parameters.parameter_name, Analyses.p_value 
FROM Analyses 
JOIN Parameters  ON Analyses.parameter_id = Parameters.parameter_id 
JOIN Genes ON Analyses.gene_accession_id = Genes.gene_accession_id;
```

```{sql}
#| eval: false
-- This query retrieves the names of parameters associated with the group named "brain."
SELECT parameter_name 
FROM Parameters 
WHERE parameter_id IN (
  SELECT parameter_id     
  FROM ParameterGroupings     
  WHERE group_id IN (         
    SELECT group_id         
    FROM Groupings         
    WHERE group_name = "brain") );
```

```{r}
#| eval: false

con <- dbConnect(
  RMySQL::MySQL(),
  dbname = "IMPCDb",  # Replace with your database name
  host = "localhost",
  port = 3306,
  user = "root",
  password = "pw"  # Replace with your password
)
# Get the options for the drop-downs 

# Mouse strain
mouse_strains <- dbGetQuery(con, "SELECT DISTINCT mouse_strain FROM Analyses") # Query unique values 
mouse_strains <- c("All", sort(mouse_strains$mouse_strain))  # Sort and prepend "All"

# Life stage
life_stages <- dbGetQuery(con, "SELECT DISTINCT mouse_life_stage FROM Analyses")
life_stages <- c("All", sort(life_stages$mouse_life_stage))

# Reactive function to construct the query based on user input
# -- This query retrieves the average p-value for each combination of gene and parameter, filtered by mouse strain and life stage (based on user inputs in the Shiny app), and orders the results by the average p-value in ascending order.
data <- reactive({
  # Start the base query
  query <- "SELECT gene_accession_id, parameter_id, ROUND(AVG(p_value), 6) AS avg_p_value FROM Analyses WHERE 1=1"
  
  # Add mouse_life_stage condition if not "All"
  if (input$life_stage != "All") {
    query <- paste0(query, " AND mouse_life_stage = '", input$life_stage, "'")
  }
  
  # Add mouse_strain condition if not "All"
  if (input$mouse_strain != "All") {
    query <- paste0(query, " AND mouse_strain = '", input$mouse_strain, "'")
  }
  
  # Add GROUP BY clause
  query <- paste0(query, " GROUP BY gene_accession_id, parameter_id ORDER BY avg_p_value ASC;")
  
  # Execute the query and return the result
  dbGetQuery(con, query)
})

# Pivot to a gene-by-parameter matrix for PCA
pca_matrix <- reactive({
  data() %>%
    pivot_wider(names_from = parameter_id, values_from = avg_p_value) %>%
    column_to_rownames(var = "gene_accession_id")  # Set gene_accession_id as row names
})
```

```{sql}
#| eval: false

-- This query retrieves information about genes, their associated parameters, and the average p-value of statistical analyses, while including the parameter names for better interpretation.
SELECT 
    a.gene_accession_id, 
    a.parameter_id, 
    p.parameter_name, 
    ROUND(AVG(a.p_value), 6) AS avg_p_value
FROM 
    Analyses a
JOIN 
    Parameters p
ON 
    a.parameter_id = p.parameter_id
WHERE 
    1=1
GROUP BY 
    a.gene_accession_id, 
    a.parameter_id, 
    p.parameter_name
ORDER BY 
    avg_p_value ASC;

```

## **Examining PCA Loadings & Identifying Parameter Groupings**

Below is a **self-contained script** that (1) reads data from the database, (2) pivots it wide, (3) runs PCA, (4) extracts loadings, and (5) clusters parameters (variables) by their loadings.

```{r}
#| eval: false

# Load necessary libraries
library(DBI)
library(RMySQL)
library(tidyr)
library(tibble)
library(dplyr)
library(ggplot2)

# Connect to the MySQL database
con <- dbConnect(
  RMySQL::MySQL(),
  dbname = "IMPCDb",  # Replace with your database name
  host = "localhost",
  port = 3306,
  user = "root",
  password = "pw"  # Replace with your password
)

# Query the database
query <- "
SELECT 
    gene_accession_id, 
    parameter_id, 
    ROUND(AVG(p_value), 6) AS avg_rounded_pvalue
FROM 
    Analyses
WHERE 
    p_value IS NOT NULL
GROUP BY 
    gene_accession_id, parameter_id
ORDER BY 
    avg_rounded_pvalue ASC;
"
genes_and_param <- dbGetQuery(con, query)

# Disconnect the database connection
dbDisconnect(con)

# Check the structure of the imported data
str(genes_and_param)

#Checking that there are no duplicates  
duplicates <- genes_and_param %>%
  dplyr::count(gene_accession_id, parameter_id) %>%
  dplyr::filter(n > 1)

print(duplicates)

# Reshape data into a wide format (gene_accession_id as rows, parameter_id as columns)
genes_param_wide <- genes_and_param %>%
  pivot_wider(
    names_from = parameter_id, # Use parameter_id as column headers
    values_from = avg_rounded_pvalue, #Fill cells with avg_p_value
    values_fill = list(avg_rounded_pvalue = 0)  # Fill missing values with 0
  )

head(genes_param_wide) # Check structure 

# Save the reshaped matrix as needed for gene clustering
write.csv(genes_param_wide, "/Users/sama/Desktop/DCDM/data/groupings//genes_phenotype_scores.csv", row.names = FALSE)

# Read the reshaped matrix
genes_param_wide <- read.csv("/Users/sama/Desktop/DCDM/data/groupings//genes_phenotype_scores.csv")

# Prepare the data for PCA (exclude the first column containing gene_accession_id)
pca_input <- genes_param_wide %>% 
  select(-gene_accession_id) %>% 
  as.matrix()

write.csv(pca_input, "/Users/sama/Desktop/DCDM/data/groupings/pca_input.csv", row.names = FALSE)

# Perform PCA
pca_result <- prcomp(pca_input, scale. = TRUE)

# View the explained variance by each principal component
summary(pca_result)$importance[2, ]  # proportion of variance explained

# =============================================================================
#    Examine the PCA Loadings
#    - This is what you need to group PARAMETERS by their shared variance
# =============================================================================
loadings <- pca_result$rotation  # rows=parameters, columns=PCs
head(loadings)

# Example: see top loadings for PC1
# sort them in descending order
pc1_loadings <- sort(loadings[, "PC1"], decreasing = TRUE)
head(pc1_loadings)

# Barplots for each PC:
barplot(loadings[, "PC1"], las = 2, main = "Loadings for PC1")


# Or create a heatmap of all loadings:
pheatmap(loadings,
         cluster_rows = TRUE, 
         cluster_cols = TRUE,
         main = "Heatmap of PCA Loadings (Parameters x PCs)")

# =============================================================================
#    Group Parameters by Their Loadings
#    For instance, cluster the rows of 'loadings'
# =============================================================================

# option 1: K-means on loadings
set.seed(123) # For reproducibility
num_parameter_clusters <- 6  # choose how many param clusters you want
km_loadings <- kmeans(loadings, centers = num_parameter_clusters)

# Check cluster assignments
param_cluster_assignments <- data.frame(
  parameter_id = rownames(loadings),
  cluster      = km_loadings$cluster
)

head(param_cluster_assignments)

# option 2: Alternatively, hierarchical clustering on loadings
dist_mat <- dist(loadings)
hc <- hclust(dist_mat, method = "ward.D2")
plot(hc, main = "Clustering of Parameters by PCA Loadings", xlab = "", sub = "")

# =============================================================================
#    Interpret the Groups
#    - Look at which parameters ended up in each cluster
#    - Possibly rename them, e.g., "Neurological-like group", "Metabolic group", etc.
# =============================================================================

View(param_cluster_assignments)  # see which parameters are in each group

# Saving the parameter groupings
write.csv(param_cluster_assignments, "parameter_clusters.csv", row.names=FALSE)


```

The param_cluster_assignments dataframe maps each parameter (parameter_id) to a cluster number, thus telling us which parameters are grouped together based on their similar loain

PCA trial (while Shiny wasn't working):

```{r}
#| eval: false

# Add PCA-transformed data to a data frame
pca_data <- as.data.frame(pca_result$x)
pca_data$gene_accession_id <- genes_param_wide$gene_accession_id

# Perform K-means clustering on PCA-transformed data
set.seed(123)  # Set seed for reproducibility
num_clusters <- 3  # Choose the number of clusters (adjust as needed)
kmeans_result <- kmeans(pca_data[, 1:2], centers = num_clusters)

# Add cluster information to the PCA data frame
pca_data$cluster <- as.factor(kmeans_result$cluster)

# Visualize the PCA results with clusters
ggplot(pca_data, aes(x = PC1, y = PC2, color = cluster, label = gene_accession_id)) +
  geom_point(size = 3, alpha = 0.8) +
  scale_color_manual(values = c("indianred3", "steelblue", "darkolivegreen3")) +  # Customize cluster colors
  labs(
    title = "PCA Clustering of Genes Based on Phenotype Scores",
    x = "Principal Component 1",
    y = "Principal Component 2",
    color = "Cluster"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 12, face = "bold"),
    axis.text = element_text(size = 10),
    legend.position = "right",
    panel.border = element_rect(color = "black", fill = NA, size = 1.5),
    panel.grid.major = element_line(size = 0.5, linetype = "dotted", color = "gray80"),
    panel.grid.minor = element_blank()
  ) +
  geom_text(aes(label = gene_accession_id), vjust = 1.5, size = 3, check_overlap = TRUE)  # Optional: Add gene labels

```

**Identifying Parameter Groupings**

```{r}
#| eval: false

## Extract Loadings 
loadings <- pca_result$rotation  # This is a matrix of size (num_parameters √ó num_PCs)
head(loadings)

# Each row in loadings corresponds to a parameter, and each column is a principle component 
# A ‚Äúhigh loading‚Äù (positive or negative) in, say, PC1 means that parameter strongly influences the first principal component

## Inspecting loadings

# Bar plot for a single PC.
barplot(loadings[, "PC1"], las = 2,
        main = "Loadings for PC1",
        ylab = "Loading Value")


## Grouping parameters based on their loadings
param_clusters <- kmeans(loadings, centers = 4) 
# or hierarchical clustering on 'loadings' to find sets of parameters that have similar loading patterns across the PCs

```

**Create groupings based on clusters**

```{r}
#| eval: false

write.csv(param_cluster_assignments, "/Users/sama/Desktop/DCDM/data/groupings/parameter_clusters.csv", row.names = FALSE)

write.csv(parameters_table, "/Users/sama/Desktop/DCDM/data/groupings/parameter_table.csv", row.names = FALSE)


# Load the cluster assignments and the parameter name mapping
param_clusters <- read.csv("/Users/sama/Desktop/DCDM/data/groupings/parameter_clusters.csv")
parameter_names <- read.csv("/Users/sama/Desktop/DCDM/data/groupings/parameter_table.csv")

ma <- read.csv("/Users/sama/Desktop/DCDM/data/cleaned/cleaned_IMPC_parameter_description.csv")
view(ma)

# Select only the 'parameter_id' and 'parameter_name' columns from parameter_names
parameter_names <- parameter_names %>%
  select(parameter_id, parameter_name)

# Merge the clusters with parameter names
param_with_names <- param_clusters %>%
  left_join(parameter_names, by = "parameter_id")

# Check the first few rows to ensure the join worked
param_with_names


```

```{r}
#| eval: false

cluster_summary <- param_with_names %>%
  group_by(cluster) %>%
  summarise(num_parameters = n())
```

```{r}
#| eval: false

grouped_parameters <- param_with_names %>%
  group_by(cluster) %>%
  summarise(
    parameter_names = paste(parameter_name, collapse = ", "),  # Combine all parameter names in a cluster
    .groups = 'drop'
  )

grouped_parameters_separated <- param_with_names %>%
  select(cluster, parameter_name) %>%  
  arrange(cluster)  

# Now pivot the data to have clusters as columns and parameters as rows
grouped_parameters_wide <- grouped_parameters_separated %>%
  group_by(cluster) %>%
  mutate(row_id = row_number()) %>%  # Create a row identifier to align the values across clusters
  ungroup() %>%
  pivot_wider(
    names_from = cluster,          # Cluster values become column names
    values_from = parameter_name,  # Parameter names become values in the columns
    values_fn = list(parameter_name = ~ .), # Make sure each parameter is treated as a separate list entry
    values_fill = list(parameter_name = NA)  # Fill missing values with NA if there are different row counts per cluster
  )

# Save the grouped parameter
write.csv(grouped_parameters_wide, "/Users/sama/Desktop/DCDM/data/groupings/grouped_parameters_wide.csv", row.names = FALSE)

```

Examining the data:

**Cluster 1:** behavioral and neurological features, Eye, morphology, metabolic (Body, size and weights) and biochemical, immune system/hematological features.

**Cluster 2:** blood and immune system, Metabolic function (organ health and energy balance), Neurological (brain, behavior, and cognitive function), Morphology and external appearance (body and organ size, bone structure, skin, hair, and ear morphology) Pathological markers, Eye.

**Cluster 3:** External appearance (Skin colour), Audiology, Metabolism, Blood, Morphology, Cardiovascular, Behavioral features, Bones and skeletal, Immune cells and Eye.

**Cluster 4:** Metabolism, Cardiovascular, External appearance, Audiology, Neurological and Eye

**Cluster 5:** Lipid metabolism and cholesterol, Eye, Bones and joint health, Blood, Immune system, growth and development, Behavioral and neurological, External appearance.

**Cluster 6:** Immune system, external appearance, Eye, Physiology and their measurements, Morphology, Behavioral responses.

**Possible groupings:** Metabolism, Eye, Cardiovascular,Morphology, Audiology, Immunology, Behaviorial responses.
